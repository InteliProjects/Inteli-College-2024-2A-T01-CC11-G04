{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQtsPgODg9zu"
      },
      "source": [
        "# Importando Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECgsHnET5pOm",
        "outputId": "60d3ec7c-105a-46e9-9806-6b41a1c8bdf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv1dSL0Qd6l-",
        "outputId": "bdc40fa3-5618-414f-a942-03acc7277bea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jedi\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi) (0.8.4)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install jedi\n",
        "!pip install -U spacy -q\n",
        "!pip install -U pip setuptools wheel -q\n",
        "!python -m spacy download pt_core_news_sm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JqgHiqKZGkTy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string \n",
        "import pandas as pd\n",
        "\n",
        "from copy import Error\n",
        "from enum import Enum, auto\n",
        "from spacy.tokens import Token\n",
        "from google.colab import runtime\n",
        "from nltk.stem import RSLPStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Type, Union \n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.xx import MultiLanguage\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from multiprocessing import Pool, cpu_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbkTeTn57GUF",
        "outputId": "0fc8cc8f-76a3-45b5-8827-0d0d3ce47987"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('rslp')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGVERaoMhAjY"
      },
      "source": [
        "# Pré-Processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WMHLzLW0vuW"
      },
      "source": [
        "## Definição da Pipeline\n",
        "\n",
        "Criação de uma classe abstrata para as etapas poderem usar como interface durante a sua instanciação, foi criado está interface para garantir a manutenção e legibilidade do código.\n",
        "\n",
        "Outrora, a classe \"PreprocessingTextPipeline\" será usada para instanciação para a criação da pipeline em si, permitindo adicionar etapas e garantir a execução.\n",
        "\n",
        "Portanto, vale lembrar que existe duas opções de bibliotecas usadas, no caso são NLTK e Spacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8vXTk2Yz_iDZ"
      },
      "outputs": [],
      "source": [
        "class NLPLibrary(Enum):\n",
        "    \"\"\"\n",
        "    Enum for supported NLP libraries.\n",
        "    \"\"\"\n",
        "    nltk = 0\n",
        "    spacy = 1\n",
        "\n",
        "class TextProcessingStep(ABC):\n",
        "    \"\"\"\n",
        "    Base class for a text processing step.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def execute(self, data: Union[str, List[str]]) -> Union[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Executes the processing step.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def requires(self) -> List[Type['TextProcessingStep']]:\n",
        "        \"\"\"\n",
        "        Specifies steps that must be executed before this one.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class PreprocessingTextPipeline:\n",
        "    \"\"\"\n",
        "    Pipeline for managing and executing text preprocessing steps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nlpLibrary: NLPLibrary):\n",
        "        \"\"\"\n",
        "        Initializes the pipeline.\n",
        "        \"\"\"\n",
        "        self.steps = []\n",
        "        self.nlpLibrary = nlpLibrary\n",
        "\n",
        "    def add_step(self, step: TextProcessingStep):\n",
        "        \"\"\"\n",
        "        Adds a step to the pipeline, checking dependencies.\n",
        "        \"\"\"\n",
        "        for required_step in step.requires():\n",
        "            if not any(isinstance(s, required_step) for s in self.steps):\n",
        "                raise ValueError(f\"{step.__class__.__name__} requires {required_step.__name__} to be added first.\")\n",
        "\n",
        "        self.steps.append(step)\n",
        "\n",
        "    def run(self, data: Union[str, List[str]]) -> Union[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Executes all steps in the pipeline.\n",
        "        \"\"\"\n",
        "        for step in self.steps:\n",
        "            data = step.execute(data, self.nlpLibrary)\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93btEjZ409yy"
      },
      "source": [
        "## Definição das etapas da Pipeline\n",
        "\n",
        "Descrição das etapas e seus propósitos:\n",
        "\n",
        "**Limpeza dos Dados**\n",
        "   - **RemoveEscapeSequences**: Esta etapa remove sequências de escape, como quebras de linha, tabulações, eliminando caracteres desnecessários que não contribuem para o significado semântico do texto.\n",
        "   - **RemoveNumbers**: Esta classe remove dígitos numéricos do texto usando expressões regulares, limpando os dados para focar no conteúdo textual.\n",
        "   - **RemovePunctuation**: Esta etapa elimina sinais de pontuação, que geralmente não têm valor semântico significativo e podem introduzir ruído no processamento do texto.\n",
        "\n",
        "**Normalização**\n",
        "   - **Lowercase**: Converte todos os caracteres do texto para letras minúsculas, garantindo uniformidade e prevenindo discrepâncias devido à capitalização. Essa normalização é crucial para manter a consistência na representação das palavras.\n",
        "\n",
        "**Tokenização**\n",
        "   - **Tokenization**: O texto é segmentado em unidades menores chamadas tokens, que podem ser palavras ou frases. Essa segmentação permite que o modelo de NLP processe o texto de maneira mais granular, facilitando a análise subsequente, como stemming ou lematização.\n",
        "\n",
        "**Remoção de Stop Words**\n",
        "   - **RemoveStopwords**: Palavras comuns que não agregam muito valor semântico, como \"e\", \"ou\" e \"é\", são removidas. Esta etapa reduz a dimensionalidade do texto e enfatiza palavras mais significativas, melhorando a eficiência computacional e a precisão do modelo.\n",
        "\n",
        "**Stemming ou Lemmatização**\n",
        "   - **Stemming**: Esta etapa reduz as palavras às suas formas raiz. Por exemplo, \"correndo\", \"corredor\" e \"correu\" seriam todos reduzidos a \"corr\". Isso ajuda o modelo a reconhecer diferentes formas de uma palavra como o mesmo conceito.\n",
        "   - **Lemmatization**: Similar ao stemming, mas mais sofisticada, a lematização reduz as palavras à sua forma base ou de dicionário, considerando o contexto e garantindo que a palavra mantenha seu significado.\n",
        "\n",
        "**Referências:**\n",
        "\n",
        "https://spotintelligence.com/2022/12/21/nltk-preprocessing-pipeline/\n",
        "\n",
        "https://www.nltk.org/\n",
        "\n",
        "https://spacy.io/api/doc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LTUiJbdl1Aip"
      },
      "outputs": [],
      "source": [
        "class RemoveEscapeSequences(TextProcessingStep):\n",
        "    \"\"\"\n",
        "    Removes escape sequences from the text.\n",
        "    \"\"\"\n",
        "    def execute(self, data: str, library: NLPLibrary) -> str:\n",
        "        return data.translate(str.maketrans({'\\n': ' ', '\\t': ' ', '\\r': ' '}))\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return []\n",
        "\n",
        "class Lowercase(TextProcessingStep):\n",
        "    \"\"\"\n",
        "    Converts all text to lowercase.\n",
        "    \"\"\"\n",
        "    def execute(self, data: str, library: NLPLibrary) -> str:\n",
        "        return data.lower()\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return []\n",
        "\n",
        "class RemoveNumbers(TextProcessingStep):\n",
        "    \"\"\"\n",
        "    Removes numbers from the text.\n",
        "    \"\"\"\n",
        "    def execute(self, data: str, library: NLPLibrary) -> str:\n",
        "        return re.sub(r'\\d+', '', data)\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return []\n",
        "\n",
        "class RemovePunctuation(TextProcessingStep):\n",
        "    \"\"\"\n",
        "    Removes punctuation from the text.\n",
        "    \"\"\"\n",
        "    def execute(self, data: str, library: NLPLibrary) -> str:\n",
        "        translator = str.maketrans('', '', string.punctuation)\n",
        "        return data.translate(translator)\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return []\n",
        "\n",
        "class Tokenization(TextProcessingStep):\n",
        "    \"\"\"\n",
        "    Tokenizes the text into words.\n",
        "    \"\"\"\n",
        "    def execute(self, data: str, library: NLPLibrary) -> List[str]:\n",
        "        if library == NLPLibrary.nltk:\n",
        "            return word_tokenize(data)\n",
        "        elif library == NLPLibrary.spacy:\n",
        "            tokens = MultiLanguage().tokenizer(data)\n",
        "            return [token.text for token in tokens]\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return []\n",
        "\n",
        "class RemoveStopwords(TextProcessingStep):\n",
        "    \"\"\"\n",
        "    Removes stopwords from the text.\n",
        "    \"\"\"\n",
        "    def execute(self, data: List[str], library: NLPLibrary) -> List[str]:\n",
        "        if library == NLPLibrary.nltk:\n",
        "            stop_words = set(stopwords.words('portuguese'))\n",
        "            return [word for word in data if word.lower() not in stop_words]\n",
        "        elif library == NLPLibrary.spacy:\n",
        "            nlp = spacy.load(\"pt_core_news_sm\")\n",
        "            doc = nlp.make_doc(\" \".join(data))\n",
        "            return [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return [Tokenization]\n",
        "\n",
        "class Stemming(TextProcessingStep):\n",
        "    \"\"\"\n",
        "    Applies stemming to the text.\n",
        "    \"\"\"\n",
        "    def execute(self, data: List[str], library: NLPLibrary) -> List[str]:\n",
        "        if library == NLPLibrary.nltk:\n",
        "            stemmer = RSLPStemmer()\n",
        "            return [stemmer.stem(word) for word in data]\n",
        "        elif library == NLPLibrary.spacy:\n",
        "            raise Error(\"spacy does not provide built-in stemming resources\")\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return [Tokenization, RemoveStopwords]\n",
        "\n",
        "class Lemmatization(TextProcessingStep):\n",
        "    \"\"\"\n",
        "    Applies lemmatization to the text.\n",
        "    \"\"\"\n",
        "    def execute(self, data: List[str], library: NLPLibrary) -> List[str]:\n",
        "        if library == NLPLibrary.nltk:\n",
        "            raise Error(\"nltk does not provide built-in pt-br lemmatizer resources\")\n",
        "        elif library == NLPLibrary.spacy:\n",
        "            nlp = spacy.load(\"pt_core_news_sm\")\n",
        "            doc = nlp(\" \".join(data))\n",
        "            return [token.lemma_ for token in doc]\n",
        "\n",
        "    def requires(self) -> List[Type[TextProcessingStep]]:\n",
        "        return [Tokenization, RemoveStopwords]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhymu92i28ou"
      },
      "source": [
        "## Definição da sequência na Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H4a1TU3YhNpG"
      },
      "outputs": [],
      "source": [
        "# Build the sequence that belongs to Pipeline\n",
        "pipeline = PreprocessingTextPipeline(NLPLibrary.spacy)\n",
        "\n",
        "pipeline.add_step(RemoveEscapeSequences())\n",
        "pipeline.add_step(Lowercase())\n",
        "pipeline.add_step(RemoveNumbers())\n",
        "pipeline.add_step(RemovePunctuation())\n",
        "pipeline.add_step(Tokenization())\n",
        "pipeline.add_step(RemoveStopwords())\n",
        "pipeline.add_step(Lemmatization())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pfDYtzi2h5R"
      },
      "source": [
        "## Processamento da base de dados\n",
        "\n",
        "Nesta seção, é mostrado o processamento da base de dados ao longo da pipeline. No final, uma pequena demonstração da base de dados processada num dataframe.\n",
        "\n",
        "Observação: Na variável \"path_database\" substitua o valor desta pelo caminho que se encontra a base de dados no seu dispositivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rG0LTlzLL7-5"
      },
      "outputs": [],
      "source": [
        "def process_question(question):\n",
        "    # Process a question using the NLP pipeline and join the results with commas.\n",
        "    return ','.join(pipeline.run(question))\n",
        "\n",
        "path_database = \"/content/drive/MyDrive/data.xlsx\"  # Path to the Excel file.\n",
        "\n",
        "df = pd.read_excel(path_database)  # Load the Excel file into a DataFrame.\n",
        "\n",
        "df.columns = ['no', 'intention', 'question', 'answers']  # Rename the DataFrame columns.\n",
        "\n",
        "df = df.drop(columns='answers')  # Drop the 'answers' column as it's not needed.\n",
        "\n",
        "with Pool(cpu_count()) as pool:\n",
        "    # Process each question in parallel using the CPU cores.\n",
        "    df['processed_question'] = pool.map(process_question, df['question'])\n",
        "\n",
        "processed_df = df[['intention', 'question', 'processed_question']]  # Select relevant columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "OLWCZt10Op8D",
        "outputId": "2aabaf3a-79c2-4984-df6c-96824447ed27"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"processed_df\",\n  \"rows\": 505,\n  \"fields\": [\n    {\n      \"column\": \"intention\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"Como depositar\",\n          \"Como fazer remessa\",\n          \"Solicitacao de cartao de remessas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 437,\n        \"samples\": [\n          \"Ol\\u00e1 bom dia, fiz a solicita\\u00e7\\u00e3o de uma remessa para o Brasil e ainda n\\u00e3o recebi o dep\\u00f3sito do valor\",\n          \"Boa tarde\\n250 reais\\nQuanto preciso depositar correio\\nOnegay\",\n          \"Bom dia. Fiz remessa para o Vietn\\u00e3 hoje \\u00e0s 11:11 AM e o benefici\\u00e1rio ainda n\\u00e3o recebeu. Gostaria de saber o motivo. grato\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 434,\n        \"samples\": [\n          \"fiz,pedir,reembolso,gostar,demor,cair,conta,correio\",\n          \"   ,real,preciso,depositar,correio,onegay\",\n          \"dia,poder,passar,passo,passo,remesso\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "processed_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-381aab3b-718c-454a-a611-ca96aeaabf9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>intention</th>\n",
              "      <th>question</th>\n",
              "      <th>processed_question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Como depositar</td>\n",
              "      <td>Boa dia.tudo bem?eu gostaria de saber sobre aq...</td>\n",
              "      <td>diatudo,bemeu,gostar,caixa,family,Mart,verde,e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Como fazer remessa</td>\n",
              "      <td>Como enviar dinheiro do Japão?</td>\n",
              "      <td>enviar,dinheiro,Japão</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tempo de remessa</td>\n",
              "      <td>Quanto tempo levará para o beneficiário recebe...</td>\n",
              "      <td>levar,beneficiário,receber,dinheiro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pedido de envio via metodo \"ByPhone\"</td>\n",
              "      <td>Boa tarde\\nAcabei de fazer a transferência de ...</td>\n",
              "      <td>acabar,transferência,   ,total,Yenes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pedido de envio via metodo \"ByPhone\"</td>\n",
              "      <td>Poderia fazer a remessa de 22yenes para o BBB ...</td>\n",
              "      <td>poder,remesso,yenes,bbb,rrr,    ,yenes,Aaaa,Mm...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-381aab3b-718c-454a-a611-ca96aeaabf9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-381aab3b-718c-454a-a611-ca96aeaabf9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-381aab3b-718c-454a-a611-ca96aeaabf9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5caa304a-1ba1-4009-a13a-02a74acc3342\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5caa304a-1ba1-4009-a13a-02a74acc3342')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5caa304a-1ba1-4009-a13a-02a74acc3342 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                              intention  \\\n",
              "0                        Como depositar   \n",
              "1                    Como fazer remessa   \n",
              "2                      Tempo de remessa   \n",
              "3  Pedido de envio via metodo \"ByPhone\"   \n",
              "4  Pedido de envio via metodo \"ByPhone\"   \n",
              "\n",
              "                                            question  \\\n",
              "0  Boa dia.tudo bem?eu gostaria de saber sobre aq...   \n",
              "1                     Como enviar dinheiro do Japão?   \n",
              "2  Quanto tempo levará para o beneficiário recebe...   \n",
              "3  Boa tarde\\nAcabei de fazer a transferência de ...   \n",
              "4  Poderia fazer a remessa de 22yenes para o BBB ...   \n",
              "\n",
              "                                  processed_question  \n",
              "0  diatudo,bemeu,gostar,caixa,family,Mart,verde,e...  \n",
              "1                              enviar,dinheiro,Japão  \n",
              "2                levar,beneficiário,receber,dinheiro  \n",
              "3               acabar,transferência,   ,total,Yenes  \n",
              "4  poder,remesso,yenes,bbb,rrr,    ,yenes,Aaaa,Mm...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the head of dataframe for demonstration\n",
        "processed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Aq9tSORuOr8X"
      },
      "outputs": [],
      "source": [
        "# Export data for the excel format\n",
        "processed_df.to_excel('processed_data.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWhXEkvWuRFa"
      },
      "source": [
        "# Testes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RUC897UovEOe"
      },
      "outputs": [],
      "source": [
        "import unittest # Library for doing unit and integrate tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeixEd3Bx1in"
      },
      "source": [
        "A partir da construção da pipeline, para a construção dos testes foi usado a biblioteca \"unittest\" e a frase abaixo , retirada da base de dados:\n",
        "\n",
        "**\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\"**\n",
        "\n",
        "Para os testes, foram feitas duas seções, visto que a primeira se trata dos testes unitários e a segunda dos dois testes de integração."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-zwM6ODyEYi"
      },
      "source": [
        "### Testes Unitários\n",
        "\n",
        "As funções dos testes unitários para serem testados e seus resultados esperados, estão logo abaixo:\n",
        "\n",
        "- TestRemoveEscapeSequences (remove sequências de escape):\n",
        "\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\" -> \"Bom dia, quanto da 150.000 yenes no Brasil hj ?\"\n",
        "\n",
        "- TestLowercase (sentença em caixa baixa):\n",
        "\n",
        "\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\" -> \"bom dia, quanto da 150.000 yenes no brasil hj ?\"\n",
        "\n",
        "- TestRemoveNumbers (Remoção de números na sentença):\n",
        "\n",
        "\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\" -> \"Bom dia, quanto da . yenes no Brasil hj ?\"\n",
        "\n",
        "- TestRemovePunctuation (Remoção de sinais de pontuação):\n",
        "\n",
        "\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\" -> \"Bom dia quanto da 150000 yenes no Brasil hj\"\n",
        "\n",
        "- TestTokenization (Segmentação do texto em unidades menores chamadas tokens) :\n",
        "\n",
        "\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\" -> ['Bom', 'dia', ',', 'quanto', 'da', '150.000', 'yenes', 'no', 'Brasil', 'hj', '?']\n",
        "\n",
        "- TestRemoveStopwords (Palavras que são removidas do texto para reduzir a dimensionalidade e focar nas palavras de maior relevância):\n",
        "\n",
        "['Bom', 'dia', ',', 'quanto', 'da', '150.000', 'yenes', 'no', 'Brasil', 'hj', '?'] -> ['Bom', 'dia', ',', 'quanto', '150.000', 'yenes', 'Brasil', 'hj', '?']\n",
        "\n",
        "- TestStemming (Conversão de palavras para seu radical base):\n",
        "\n",
        "['Bom', 'dia', ',', 'quanto', 'da', '150.000', 'yenes', 'no', 'Brasil', 'hj', '?'] -> ['Bom', 'dia', ',', 'quant', 'da', '150.000', 'yen', 'no', 'Brasil', 'hj', '?']\n",
        "\n",
        "- TestLemmatization (Conversão de palavras para seus lemas):\n",
        "\n",
        "['Bom', 'dia', ',', 'quanto', 'da', '150.000', 'yenes', 'no', 'Brasil', 'hj', '?'] -> ['Bom', 'dia', ',', 'quanto', 'de o', '150.000', 'yene', 'em o', 'Brasil', 'hj', '?']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4idsXjvlyGGz"
      },
      "outputs": [],
      "source": [
        "class TestRemoveEscapeSequences(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit test for the removal of escape sequences from text.\n",
        "    \"\"\"\n",
        "\n",
        "    def test_remove_escape_sequences(self):\n",
        "        \"\"\"\n",
        "        Tests whether the `execute` method of the `RemoveEscapeSequences` class correctly removes escape sequences.\n",
        "        \"\"\"\n",
        "        step = RemoveEscapeSequences()  # Instantiate the RemoveEscapeSequences class\n",
        "        self.assertEqual(step.execute(\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\", NLPLibrary.nltk),\n",
        "                         \"Bom dia, quanto da 150.000 yenes no Brasil hj ?\")\n",
        "\n",
        "class TestLowercase(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit test for the text conversion to lowercase step.\n",
        "    \"\"\"\n",
        "\n",
        "    def unit_test_lowercase(self):\n",
        "        \"\"\"\n",
        "        Tests whether the `execute` method of the `Lowercase` class correctly converts text to lowercase.\n",
        "        \"\"\"\n",
        "        step = Lowercase()  # Instantiate the Lowercase class\n",
        "        self.assertEqual(step.execute(\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\", NLPLibrary.nltk),\n",
        "                         \"bom dia, quanto da 150.000 yenes no brasil hj ?\")\n",
        "\n",
        "\n",
        "class TestTokenization(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit test for the text tokenization step.\n",
        "    \"\"\"\n",
        "\n",
        "    def unit_test_tokenization(self):\n",
        "        \"\"\"\n",
        "        Tests whether the `execute` method of the `Tokenization` class correctly splits the text into tokens.\n",
        "        \"\"\"\n",
        "        step = Tokenization()  # Instantiate the Tokenization class\n",
        "        self.assertEqual(step.execute(\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\", NLPLibrary.nltk),\n",
        "                         ['Bom', 'dia', ',', 'quanto', 'da', '150.000', 'yenes', 'no', 'Brasil', 'hj', '?'])\n",
        "\n",
        "class TestRemoveNumbers(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit test for the text numbers removal step.\n",
        "    \"\"\"\n",
        "\n",
        "    def unit_test_remove_numbers(self):\n",
        "        \"\"\"\n",
        "        Tests whether the `execute` method of the `RemoveNumbers` class correctly removes numbers from the text.\n",
        "        \"\"\"\n",
        "        step = RemoveNumbers()  # Instantiate the RemoveNumbers class\n",
        "        self.assertEqual(step.execute(\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\", NLPLibrary.nltk),\n",
        "                         \"Bom dia, quanto da . yenes no Brasil hj ?\")\n",
        "\n",
        "\n",
        "class TestRemovePunctuation(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit test for the text punctuation removal step.\n",
        "    \"\"\"\n",
        "\n",
        "    def unit_test_remove_punctuation(self):\n",
        "        \"\"\"\n",
        "        Tests whether the `execute` method of the `RemovePunctuation` class correctly removes punctuation from the text.\n",
        "        \"\"\"\n",
        "        step = RemovePunctuation()  # Instantiate the RemovePunctuation class\n",
        "        self.assertEqual(step.execute(\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\", NLPLibrary.nltk),\n",
        "                         \"Bom dia quanto da 150000 yenes no Brasil hj\")\n",
        "\n",
        "class TestRemoveStopwords(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit test for the text stopwords removal step.\n",
        "    \"\"\"\n",
        "\n",
        "    def unit_test_remove_stopwords(self):\n",
        "        \"\"\"\n",
        "        Tests whether the `execute` method of the `RemoveStopwords` class correctly removes stopwords from the tokenized text.\n",
        "        \"\"\"\n",
        "        step = RemoveStopwords()  # Instantiate the RemoveStopwords class\n",
        "        self.assertEqual(step.execute(['Bom', 'dia', ',', 'quanto', 'da', '150.000', 'yenes', 'no', 'Brasil', 'hj', '?'], NLPLibrary.nltk),\n",
        "                         ['Bom', 'dia', ',', 'quanto', '150.000', 'yenes', 'Brasil', 'hj', '?'])\n",
        "\n",
        "\n",
        "class TestStemming(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit test for the text stemming step.\n",
        "    \"\"\"\n",
        "\n",
        "    def unit_test_stemming(self):\n",
        "        \"\"\"\n",
        "        Tests whether the `execute` method of the `Stemming` class correctly applies stemming to the tokenized text.\n",
        "        \"\"\"\n",
        "        step = Stemming()  # Instantiate the Stemming class\n",
        "        self.assertEqual(step.execute(['Bom', 'dia', ',', 'quanto', 'da', '150.000', 'yenes', 'no', 'Brasil', 'hj', '?'], NLPLibrary.nltk),\n",
        "                         ['Bom', 'dia', ',', 'quant', 'da', '150.000', 'yen', 'no', 'Brasil', 'hj', '?'])\n",
        "\n",
        "class TestLemmatization(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit test for the text lemmatization step.\n",
        "    \"\"\"\n",
        "\n",
        "    def unit_test_lemmatization(self):\n",
        "        \"\"\"\n",
        "        Tests whether the `execute` method of the `Lemmatization` class correctly applies lemmatization to the tokenized text.\n",
        "        \"\"\"\n",
        "        step = Lemmatization()  # Instantiate the Lemmatization class\n",
        "        self.assertEqual(step.execute(['Bom', 'dia', ',', 'quanto', 'da', '150.000', 'yenes', 'no', 'Brasil', 'hj', '?'], NLPLibrary.spacy),\n",
        "                         ['Bom', 'dia', ',', 'quanto', 'de o', '150.000', 'yene', 'em o', 'Brasil', 'hj', '?'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4273Cw7hyt3R"
      },
      "source": [
        "### Resultados\n",
        "\n",
        "A partir da execução dos testes, pode se concluir que todos os testes passaram comprovando a qualidade das etapas criadas para a pipeline.\n",
        "Logo abaixo contém observações das mudanças ocorridas na frase dentre as funções definidas:\n",
        "\n",
        "- RemoveEscapeSequences: Retirou as sequências de escape;\n",
        "\n",
        "- Lowercase: A frase de teste ficou em caixa baixa, por exemplo, \"Bom\" ficou \"bom\";\n",
        "\n",
        "- Remove Numbers: Ocorreu a retirada dos números, como por exemplo, \"150.000\" ficou \".\";\n",
        "\n",
        "- Remove Punctuation: Ocorreu a retirada de sinais de pontuação, como por exemplo, este sinal \"?\" foi retirado;\n",
        "\n",
        "- Remove Stopwords: Ocorreu a remoção de palavras que não agregariam relevância ao significado da sentença, como por exemplo, o \"no\" foi retirado;\n",
        "\n",
        "- Tokenization: Ocorreu de fato a separação da sentença em unidades de tokens, como por exemplo, \"['Bom', 'dia', ',', 'quanto', 'da', '150.000', 'yenes', 'no', 'Brasil', 'hj', '?']\";\n",
        "\n",
        "- Stemming: Algumas palavras foram diminuidas para sua base, como por exemplo \"quanto\" ficou \"quant\";\n",
        "\n",
        "- Lemmatization: Algumas palavras foram diminuidas, por exemplo \"yenes\" tirou o plural e ficou \"yene\";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSI55kENyvB8",
        "outputId": "571fa58b-bd90-4bc3-ad62-bbc015792785"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.003s\n",
            "\n",
            "OK\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "# Execution of the tests\n",
        "def run_unit_tests():\n",
        "    unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestRemoveEscapeSequences))\n",
        "    unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestLowercase))\n",
        "    unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestTokenization))\n",
        "    unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestRemoveNumbers))\n",
        "    unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestRemovePunctuation))\n",
        "    unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestRemoveStopwords))\n",
        "    unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestStemming))\n",
        "    unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestLemmatization))\n",
        "\n",
        "run_unit_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8GphdwAy0cP"
      },
      "source": [
        "### Testes Integrados\n",
        "\n",
        "Foi feito dois testes integrados levando em consideração as duas possibilidades de bibliotecas na pipeline, primeiramente, foi feito com a biblioteca \"NLKT\". E por fim, o outro teste foi feito usando a biblioteca \"Spacy\".\n",
        "\n",
        "Logo abaixo tem as transformações feitas que seguiram com exatidão todas as etapas:\n",
        "\n",
        "- Pipeline usando o NLKT:\n",
        "\n",
        "\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\" -> ['bom', 'dia', 'quant', 'yen', 'brasil', 'hj']\n",
        "\n",
        "- Pipeline usando o Spacy:\n",
        "\n",
        "\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\" -> ['dia', '   ', 'yenes', 'Brasil', 'hj']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "shtR9AZdy1bI"
      },
      "outputs": [],
      "source": [
        "class TestPreprocessingTextPipelineLibraryNLKT(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Integration test for the PreprocessingTextPipeline using the NLKT NLP library.\n",
        "    \"\"\"\n",
        "\n",
        "    def integration_test_pipeline(self):\n",
        "        \"\"\"\n",
        "        Tests the entire preprocessing pipeline with the following steps:\n",
        "        - Removing Escape Sequences\n",
        "        - Lowercasing\n",
        "        - Removing numbers\n",
        "        - Removing punctuation\n",
        "        - Tokenization\n",
        "        - Removing stopwords\n",
        "        - Stemming\n",
        "\n",
        "        The test verifies that the pipeline processes the input text correctly using the NLKT library.\n",
        "        \"\"\"\n",
        "        # Initialize the pipeline with the NLKT library\n",
        "        pipeline = PreprocessingTextPipeline(NLPLibrary.nlkt)\n",
        "\n",
        "        # Define each step in the preprocessing pipeline\n",
        "        remove_escape_step = RemoveEscapeSequences()\n",
        "        lowercase_step = Lowercase()\n",
        "        remove_numbers_step = RemoveNumbers()\n",
        "        remove_punctuation_step = RemovePunctuation()\n",
        "        tokenization_step = Tokenization()\n",
        "        remove_stopwords_step = RemoveStopwords()\n",
        "        stemming_step = Stemming()\n",
        "        lemmatization_step = Lemmatization()\n",
        "\n",
        "        # Add steps to the pipeline\n",
        "        pipeline.add_step(remove_escape_step)\n",
        "        pipeline.add_step(lowercase_step)\n",
        "        pipeline.add_step(remove_numbers_step)\n",
        "        pipeline.add_step(remove_punctuation_step)\n",
        "        pipeline.add_step(tokenization_step)\n",
        "        pipeline.add_step(remove_stopwords_step)\n",
        "        pipeline.add_step(stemming_step)\n",
        "\n",
        "        # Run the pipeline on the input text\n",
        "        result = pipeline.run(\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\")\n",
        "        expected_result = ['bom', 'dia', 'quant', 'yen', 'brasil', 'hj']\n",
        "\n",
        "        # Assert that the result matches the expected output\n",
        "        self.assertEqual(result, expected_result)\n",
        "\n",
        "class TestPreprocessingTextPipelineLibrarySpacy(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Integration test for the PreprocessingTextPipeline using the Spacy NLP library.\n",
        "    \"\"\"\n",
        "\n",
        "    def integration_test_pipeline(self):\n",
        "        \"\"\"\n",
        "        Tests the entire preprocessing pipeline with the following steps:\n",
        "        - Removing Escape Sequences\n",
        "        - Lowercasing\n",
        "        - Removing numbers\n",
        "        - Removing punctuation\n",
        "        - Tokenization\n",
        "        - Removing stopwords\n",
        "        - Lemmatization\n",
        "\n",
        "        The test verifies that the pipeline processes the input text correctly using the Spacy library.\n",
        "        \"\"\"\n",
        "        # Initialize the pipeline with the Spacy library\n",
        "        pipeline_spacy = PreprocessingTextPipeline(NLPLibrary.spacy)\n",
        "\n",
        "        # Define each step in the preprocessing pipeline\n",
        "        remove_escape_step = RemoveEscapeSequences()\n",
        "        lowercase_step = Lowercase()\n",
        "        remove_numbers_step = RemoveNumbers()\n",
        "        remove_punctuation_step = RemovePunctuation()\n",
        "        tokenization_step = Tokenization()\n",
        "        remove_stopwords_step = RemoveStopwords()\n",
        "        stemming_step = Stemming()\n",
        "        lemmatization_step = Lemmatization()\n",
        "\n",
        "        # Add steps to the pipeline\n",
        "        pipeline_spacy.add_step(remove_escape_step)\n",
        "        pipeline_spacy.add_step(lowercase_step)\n",
        "        pipeline_spacy.add_step(remove_numbers_step)\n",
        "        pipeline_spacy.add_step(remove_punctuation_step)\n",
        "        pipeline_spacy.add_step(tokenization_step)\n",
        "        pipeline_spacy.add_step(remove_stopwords_step)\n",
        "        pipeline_spacy.add_step(lemmatization_step)\n",
        "\n",
        "        # Run the pipeline on the input text\n",
        "        result = pipeline_spacy.run(\"Bom dia, quanto da 150.000 yenes no Brasil hj ?\")\n",
        "        expected_result = ['dia', '   ', 'yenes', 'Brasil', 'hj']\n",
        "\n",
        "        # Assert that the result matches the expected output\n",
        "        self.assertEqual(result, expected_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAH4VdmPzYC7"
      },
      "source": [
        "### Resultados\n",
        "\n",
        "Ambos os testes passaram demonstrando qualidade da pipeline em si."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5iOBuFdzYh1",
        "outputId": "d2ff7359-ec2d-4290-b802-6ee9c25cc875"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "# Execution of the tests\n",
        "def run_integrate_tests():\n",
        "    unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestPreprocessingTextPipelineLibraryNLKT))\n",
        "    unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestPreprocessingTextPipelineLibrarySpacy))\n",
        "\n",
        "run_integrate_tests()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
